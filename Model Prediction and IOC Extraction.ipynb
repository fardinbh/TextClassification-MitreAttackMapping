{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcaf137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bde6eb043d2477bbf5531e0e38dfc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='Add a summary of your threat using <code>#</code>:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9e25a31f674ab08a1c6876972f7da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Text Data:', layout=Layout(height='200px', width='90%'), placeholder='Enter teâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab23048dee7f4055854a0ac6796dc8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.6, continuous_update=False, description='Confidence:', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fcee745e504fa2bbe7d5f787c8854a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Model Explainability')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a10a2a656b24501802d63dcc9a161bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from iocextract import extract_iocs\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import re\n",
    "\n",
    "# Load the dictionary back from the pickle file\n",
    "with open('technique_dictMitre.pkl', 'rb') as handle:\n",
    "    loaded_technique_dict = pickle.load(handle)\n",
    "\n",
    "import os\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "\n",
    "# Function to split input text into chunks based on '#'\n",
    "def chunk_training_data(text):\n",
    "    return [chunk.strip() for chunk in text.split(\"#\") if chunk.strip()]\n",
    "\n",
    "# Model and tokenizer initialization\n",
    "model_path = './saved_distilbert_model_Sec_Tram7'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "\n",
    "def chunk_training_data(text):\n",
    "    \"\"\" Splits the provided training data sample into separate entries. \"\"\"\n",
    "    #return re.split(r'\\n', text.strip())\n",
    "    return [chunk.strip() for chunk in text.split(\"#\") if chunk.strip()]\n",
    "'''def chunk_sentences(text):\n",
    "    \"\"\" Splits the provided text data based on hyphens. \"\"\"\n",
    "    # Split by hyphen and filter out empty strings\n",
    "    return [chunk.strip() for chunk in text.split(\"#\") if chunk.strip()]'''\n",
    "def chunk_sentences(text, chunk_size=3):\n",
    "    if not isinstance(text, str):\n",
    "        print(\"Text passed to chunk_sentences is not a string:\", type(text), text)\n",
    "        return []\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [' '.join(sentences[i:i + chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "\n",
    "def predict_mitre_technique_and_extract_iocs(text):\n",
    "    \"\"\" Predicts the technique and extracts IoCs from a given text. \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    # Using a sigmoid function to get the probability of each label being positive\n",
    "    predicted_proba = logits.sigmoid()\n",
    "    # Using the confidence threshold for multi-label predictions\n",
    "    predicted_classes = [i for i, proba in enumerate(predicted_proba[0]) if proba > confidence_threshold.value]\n",
    "    iocs = extract_iocs(text)\n",
    "    return predicted_classes, iocs, predicted_proba  # Return all predicted probabilities\n",
    "\n",
    "\n",
    "def predict_proba(texts):\n",
    "    \"\"\" Provides probability estimates for the given texts. \"\"\"\n",
    "    outputs = model(**tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512))\n",
    "    return outputs.logits.softmax(dim=1).detach().numpy()\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=['Class_0', 'Class_1'])  # Replace class names accordingly\n",
    "\n",
    "def explain_prediction(text):\n",
    "    \"\"\" Provides an explanation for the prediction. \"\"\"\n",
    "    explanation = explainer.explain_instance(text, predict_proba, num_features=10)\n",
    "    return explanation.show_in_notebook()\n",
    "\n",
    "# Widgets for user input\n",
    "ti_data_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter text data here...',\n",
    "    description='Text Data:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='90%', height='200px')\n",
    ")\n",
    "\n",
    "confidence_threshold = widgets.FloatSlider(\n",
    "    value=0.6,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='Confidence:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ")\n",
    "\n",
    "model_explainability = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Model Explainability',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Processes a single chunk from the training data.\"\"\"\n",
    "    parts = re.split(r'\\s+https://', chunk)\n",
    "    \n",
    "    # Check if the chunk contains a link and can be split into the expected parts\n",
    "    if len(parts) >= 3:\n",
    "        title = parts[0]\n",
    "        link = \"https://\" + parts[1]\n",
    "        # Combining rest of the parts for description and tech_id extraction\n",
    "        rest_of_the_data = ' '.join(parts[2:])\n",
    "        description, tech_id = rest_of_the_data.rsplit(\" \", 1)\n",
    "        \n",
    "        # Check if the extracted parts are valid\n",
    "        if all([title, link, description, tech_id]):\n",
    "            return title, link, description, tech_id\n",
    "    # If the chunk doesn't fit the expected format, treat the whole chunk as a description\n",
    "    return None, None, chunk, None\n",
    "\n",
    "\n",
    "# Initialize an empty list to save IDs\n",
    "saved_ids = []\n",
    "#from IPython.core.display import display, HTML\n",
    "def on_submit_button_click(button):\n",
    "    global saved_ids  # Declare saved_ids as global to modify it\n",
    "    \n",
    "    # Clear saved_ids to avoid duplicates\n",
    "    saved_ids.clear()\n",
    "    \n",
    "    # Clear previous outputs\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Collect input data\n",
    "    text_data = ti_data_input.value\n",
    "    \n",
    "    # Process input data\n",
    "    chunks = chunk_training_data(text_data)\n",
    "    for chunk in chunks:\n",
    "        # Ensure chunk is not empty\n",
    "        technique_ids, iocs, confidence_scores = predict_mitre_technique_and_extract_iocs(chunk)  # Get multiple techniques and their confidences\n",
    "        print(\"\\n\" + \"**\"*50 + \"\\n\")\n",
    "        print(f\"Threat_Summary: {chunk}\")\n",
    "        #display(HTML(f\"<b>Threat_Summary:</b> {chunk}\"))\n",
    "        \n",
    "        for tech_id in technique_ids:\n",
    "            # Only print the predictions with confidence score > 0.90\n",
    "            if confidence_scores[0][tech_id].item() > 0.95:\n",
    "                technique_details = loaded_technique_dict.get(tech_id, {})\n",
    "                \n",
    "                print(f\"Predicted Technique ID: {tech_id}\")\n",
    "                print(f\"Confidence Score: {confidence_scores[0][tech_id].item():.2f}\")  # Display the confidence score for each predicted technique\n",
    "                print(f\"Extracted IoCs: {iocs}\")\n",
    "                \n",
    "                # Print the technique details from the loaded dictionary\n",
    "                technique_id = technique_details.get('ID', 'N/A').strip()  # Strip spaces\n",
    "                print(f\"ID: {technique_id}\")\n",
    "                saved_ids.append(technique_id)  # Save the predicted ID\n",
    "                print(f\"Technique Name: {technique_details.get('Technique Name', 'N/A')}\")\n",
    "                key = \"Technique Name's Webpage Link\"\n",
    "                print(f\"Technique Name's Webpage Link: {technique_details.get(key, 'N/A')}\")\n",
    "                print(f\"Technique Description: {technique_details.get('Technique Description', 'N/A')}\")\n",
    "                \n",
    "                # If Model Explainability is checked\n",
    "                if model_explainability.value:\n",
    "                    explain_prediction(chunk)\n",
    "                print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # After all chunks are processed, display the saved IDs\n",
    "    print(\"Saved IDs:\", saved_ids)\n",
    " \n",
    "    # Display the widgets again for subsequent use\n",
    "    display(ti_data_input, confidence_threshold, model_explainability, submit_button)\n",
    "\n",
    "\n",
    "# Link the button to the function\n",
    "submit_button.on_click(on_submit_button_click)\n",
    "\n",
    "# Initial display of the widgets\n",
    "#display(ti_data_input, confidence_threshold, model_explainability, submit_button)\n",
    "# Add this text before the input box\n",
    "intro_text = widgets.HTML(\"Add a summary of your threat using <code>#</code>:\")\n",
    "display(intro_text, ti_data_input, confidence_threshold, model_explainability, submit_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8427c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fardin/mitregpt\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/fardin/mitregpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019efd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
