{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, get_cosine_schedule_with_warmup)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "# Data Loading & Preprocessing\n",
    "#Tram Data\n",
    "data_path = '/path-to-data/'\n",
    "with open(data_path) as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "raw = pd.DataFrame(data['sentences'])\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = raw['mappings'].explode().dropna().apply(pd.Series)\n",
    "mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db79ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((raw['text'], mappings['attack_id'].str.extract(r\"(?P<attack_id>T\\d+)(\\.(?P<subclass_id>\\d+))?\")), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eabe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attack_id'] = df.apply(lambda row: row['attack_id'] if pd.isna(row['subclass_id']) else f\"{row['attack_id']}.{row['subclass_id']}\", axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to y\n",
    "csv_path = '/path-to-second-data/'\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234da7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only necessary columns\n",
    "csv_data = csv_data[['attack_id', 'text']]\n",
    "csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_ids_list = [str(attack_id).strip() for attack_id in csv_data['attack_id'].tolist()]\n",
    "attack_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc64a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_ids_count = len(attack_ids_list)\n",
    "print(\"Number of elements in attack_ids_list:\", attack_ids_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes from JSON and CSV\n",
    "final_df = pd.concat([df, csv_data], ignore_index=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['attack_id'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_of_interest = attack_ids_list\n",
    "positive_data = final_df[final_df['attack_id'].isin(classes_of_interest)]\n",
    "negative_data = final_df[final_df['attack_id'].isna()].sample(1000).fillna('none')\n",
    "data = pd.concat((positive_data, negative_data))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba67184",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = final_df[final_df['attack_id'].isin(classes_of_interest)]\n",
    "negative_data = final_df[final_df['attack_id'].isna()].sample(1000).fillna('none')\n",
    "data = pd.concat((positive_data, negative_data))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1144c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, get_cosine_schedule_with_warmup)\n",
    "cuda = torch.device('cuda')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"jackaduma/SecBERT\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38737ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokens = tokenizer(data['text'].tolist(), return_tensors='pt', padding='max_length', truncation=True, max_length=512).input_ids\n",
    "x_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Mapping\n",
    "index_to_label = dict(enumerate(data['attack_id'].unique()))\n",
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = 'index_to_label_mapping2.csv'\n",
    "\n",
    "# Save the index_to_label dictionary to the CSV file\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    fieldnames = ['Index', 'Attack_ID']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the data rows\n",
    "    for index, attack_id in index_to_label.items():\n",
    "        writer.writerow({'Index': index, 'Attack_ID': attack_id})\n",
    "\n",
    "print(f\"Index to Label mapping has been saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = {label: index for index, label in index_to_label.items()}\n",
    "label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = torch.Tensor(data['attack_id'].replace(label_to_index).to_numpy()).to(int)\n",
    "y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Data Splitting\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_tokens, y_all, test_size=0.2, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "\n",
    "def _load_data(x, y, batch_size=10, device=torch.device(\"cpu\")):\n",
    "    x_len = len(x)\n",
    "    for i in range(0, x_len, batch_size):\n",
    "        slc = slice(i, i + batch_size)\n",
    "        \n",
    "        # Convert numpy arrays to tensors and send them to the desired device\n",
    "        x_tensor = x[slc].clone().detach().to(device)\n",
    "        y_tensor = torch.tensor(y[slc]).to(device)\n",
    "        \n",
    "        yield x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0257ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_train_encoded.max() == len(set(y_train_encoded)) - 1, \"Max label should be num_labels - 1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83690321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained SecBERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"jackaduma/SecBERT\",\n",
    "    num_labels=len(set(y_train_encoded)),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    epoch_losses = []\n",
    "    for x, y in tqdm(_load_data(x_train, y_train_encoded, batch_size=10, device=device)):\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=x, attention_mask=x.ne(0).to(int), labels=y)\n",
    "        loss = outputs.loss\n",
    "        epoch_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch {epoch} loss: {mean(epoch_losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = './saved_model-name'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
